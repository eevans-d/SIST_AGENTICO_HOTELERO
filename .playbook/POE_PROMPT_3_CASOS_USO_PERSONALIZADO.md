# ğŸ¯ PROMPT 3 DEFINITIVO: BaterÃ­a de Casos de Uso Enterprise
## PERSONALIZADO PARA: SIST_AGENTICO_HOTELERO

**OBJETIVO**: Crear un conjunto EXHAUSTIVO de casos de uso realistas para validar, entrenar y documentar el comportamiento del bot o3-pro especializado en **SIST_AGENTICO_HOTELERO**.

**PROYECTO**: Sistema multi-servicio de recepcionista hotelero AI  
**Stack**: Python 3.12.3, FastAPI, Docker Compose (7 servicios)  
**Status**: Staging-ready (8.9/10 deployment readiness, 31% coverage)  
**Commit**: 97676bcc27f7f999f602432a07383ce09c5dee68  

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## ESTRUCTURA DE CADA CASO DE USO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Cada caso debe incluir:

ğŸ“‹ **METADATA**:
â”œâ”€ ID: UC-XXX (para referencia)
â”œâ”€ CategorÃ­a: [BUG|FEATURE|OPTIMIZATION|ARCHITECTURE|SECURITY|OBSERVABILITY]
â”œâ”€ Complejidad: [SIMPLE|MEDIUM|COMPLEX|EXPERT]
â”œâ”€ MÃ³dulos involucrados: [lista de archivos/servicios]
â””â”€ Tiempo estimado de resoluciÃ³n: [5min|15min|1h|4h]

ğŸ’¬ **CONVERSACIÃ“N**:
â”œâ”€ 1. Mensaje inicial del usuario (natural, contexto realista)
â”œâ”€ 2. Respuesta esperada del bot (estructurada segÃºn formato del prompt 2)
â”œâ”€ 3. Follow-up 1 (profundizaciÃ³n tÃ©cnica)
â”œâ”€ 4. Respuesta a follow-up 1
â”œâ”€ 5. Follow-up 2 (aspecto alternativo/edge case)
â””â”€ 6. Respuesta final con sÃ­ntesis

âœ… **CRITERIOS DE Ã‰XITO**:
â””â”€ MÃ©tricas objetivas para validar si el bot respondiÃ³ correctamente

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## CATEGORÃA 1: DEBUGGING DE BUGS CRÃTICOS (5 CASOS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-001: Race Condition en Session Manager ğŸ”´

**METADATA**:
- ID: UC-001
- CategorÃ­a: BUG (Concurrency)
- Complejidad: EXPERT
- MÃ³dulos: `app/services/session_manager.py`, `app/core/middleware.py`
- Tiempo estimado: 4h (debugging + testing concurrente)

**CONVERSACIÃ“N**:

**[USER - Mensaje inicial]**
```
Estamos viendo un bug muy extraÃ±o en producciÃ³n desde que escalamos a 1000 req/s. 
A veces (5% de casos aprox) los usuarios pierden el contexto de su conversaciÃ³n 
a mitad del flujo de reserva. Los logs muestran esto:

[ERROR] session_manager.update_failed key=tenant_abc:user_xyz:whatsapp conflict_version

Parece que hay algÃºn tipo de race condition cuando 2 requests del mismo usuario 
llegan casi simultÃ¡neos (ej: usuario envÃ­a mensaje rÃ¡pido + imagen). 

Â¿Puedes investigar quÃ© estÃ¡ pasando en session_manager.py?
```

**[BOT - Respuesta esperada]**
```markdown
# ğŸ› ANÃLISIS DE BUG: Race Condition en Session Updates

## ğŸ” InvestigaciÃ³n Inicial

ğŸ“ **LocalizaciÃ³n**:
- Archivo: `app/services/session_manager.py`
- MÃ©todo: `update_session()` (lÃ­neas 200-250 aprox)
- Infraestructura: Redis sin optimistic locking

ğŸ§  **RAZONAMIENTO PASO A PASO**:

**Paso 1: ReproducciÃ³n del escenario**
Usuario envÃ­a mensaje de texto + imagen casi simultÃ¡neamente:
- Request A (texto): Lee session @ t=0, modifica, escribe @ t=50ms
- Request B (imagen): Lee session @ t=10ms, modifica, escribe @ t=60ms
- Resultado: Request B sobrescribe los cambios de Request A âŒ

**Paso 2: RevisiÃ³n del cÃ³digo actual**
```python
# session_manager.py lÃ­nea ~200 (CÃ“DIGO ACTUAL PROBLEMÃTICO)
async def update_session(self, user_id: str, data: dict, tenant_id: str = None):
    session_key = self._build_key(user_id, tenant_id)
    
    # âš ï¸ PROBLEMA: Read-Modify-Write sin atomic operation
    current_raw = await self.redis.get(session_key)
    current = json.loads(current_raw) if current_raw else {}
    
    # Merge de cambios
    current.update(data)
    
    await self.redis.setex(session_key, self.ttl, json.dumps(current))
    #    ^^^^^^^^ No hay verificaciÃ³n de que 'current' no cambiÃ³ desde el GET
```

**Paso 3: IdentificaciÃ³n del anti-pattern**
Este es el clÃ¡sico problema de "Lost Update" en bases de datos:
- Thread-unsafe read-modify-write cycle
- No hay versioning ni compare-and-swap (CAS)
- Redis se usa como KV simple sin aprovechar atomic operations

**ROOT CAUSE**:  
Session updates no son atÃ³micas. MÃºltiples requests concurrentes del mismo usuario 
causan overwrites en lugar de merges incrementales.

## âœ… SOLUCIÃ“N: Queue-based Updates + Micro-batching

```python
# âœ… CÃ“DIGO REFACTORIZADO - session_manager.py

import asyncio
from typing import Dict
from collections import defaultdict

class SessionManager:
    def __init__(self, redis_client, ttl=1800):
        self.redis = redis_client
        self.ttl = ttl
        # Queue per session para serializar updates
        self._update_queues: Dict[str, asyncio.Queue] = {}
        self._update_workers: Dict[str, asyncio.Task] = {}
        self._workers_lock = asyncio.Lock()
    
    async def update_session(
        self, 
        user_id: str, 
        data: dict, 
        tenant_id: str = None
    ) -> bool:
        """
        Update session con garantÃ­a de serializaciÃ³n por usuario.
        Usa queue-based architecture para evitar race conditions.
        
        Returns:
            bool: True if update succeeded
        """
        session_key = self._build_key(user_id, tenant_id)
        
        # Crear queue y worker si no existe para este usuario
        async with self._workers_lock:
            if session_key not in self._update_queues:
                self._update_queues[session_key] = asyncio.Queue()
                self._update_workers[session_key] = asyncio.create_task(
                    self._session_update_worker(session_key)
                )
        
        # Encolar update (non-blocking para el caller)
        update_future = asyncio.Future()
        await self._update_queues[session_key].put((data, update_future))
        
        # Esperar resultado del worker
        return await update_future
    
    async def _session_update_worker(self, session_key: str):
        """
        Worker dedicado por session_key que procesa updates serialmente.
        Implementa micro-batching: si llegan N updates en <50ms, los fusiona.
        """
        queue = self._update_queues[session_key]
        
        while True:
            try:
                # Esperar primer update
                first_data, first_future = await queue.get()
                
                # Micro-batch: esperar 50ms por updates adicionales
                batch_data = [first_data]
                batch_futures = [first_future]
                
                try:
                    while len(batch_data) < 10:  # Max 10 updates por batch
                        data, future = await asyncio.wait_for(
                            queue.get(), 
                            timeout=0.05  # 50ms window
                        )
                        batch_data.append(data)
                        batch_futures.append(future)
                except asyncio.TimeoutError:
                    pass  # No mÃ¡s updates, procesar batch
                
                # Fusionar todos los updates del batch
                merged_update = {}
                for data in batch_data:
                    merged_update = self._deep_merge(merged_update, data)
                
                # Aplicar update atÃ³mico (ahora serial, no hay race)
                try:
                    current_raw = await self.redis.get(session_key)
                    current = json.loads(current_raw) if current_raw else {}
                    final = self._deep_merge(current, merged_update)
                    
                    await self.redis.setex(
                        session_key, 
                        self.ttl, 
                        json.dumps(final)
                    )
                    
                    # Resolver todos los futures del batch
                    for fut in batch_futures:
                        fut.set_result(True)
                    
                    logger.info(
                        "session_manager.batch_update_success",
                        session_key=session_key,
                        batch_size=len(batch_data)
                    )
                    
                except Exception as e:
                    # Marcar todos los updates como fallidos
                    for fut in batch_futures:
                        fut.set_exception(e)
                    
                    logger.error(
                        "session_manager.batch_update_failed",
                        error=str(e),
                        session_key=session_key
                    )
                
            except asyncio.CancelledError:
                # Worker shutdown graceful
                break
    
    def _deep_merge(self, base: dict, updates: dict) -> dict:
        """Merge nested dictionaries recursively"""
        result = base.copy()
        for key, value in updates.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        return result
    
    async def cleanup_idle_workers(self):
        """
        Cleanup workers de sesiones inactivas (llamar periÃ³dicamente).
        """
        # TODO: Implementar TTL de workers (cerrar despuÃ©s de 5min sin updates)
        pass
```

## ğŸ§ª TESTING EXHAUSTIVO

```python
# tests/integration/test_session_concurrency.py

import pytest
import asyncio
from app.services.session_manager import SessionManager

@pytest.mark.asyncio
async def test_concurrent_updates_dont_lose_data(redis_client):
    """
    Simular 10 updates concurrentes del mismo usuario.
    Verificar que TODOS los cambios se apliquen (no se pierda ninguno).
    """
    session_mgr = SessionManager(redis_client)
    user_id = "concurrent_test_user"
    tenant_id = "tenant_abc"
    
    # FunciÃ³n que actualiza un campo especÃ­fico
    async def update_field(field_name: str, value: any):
        return await session_mgr.update_session(
            user_id, 
            {field_name: value},
            tenant_id
        )
    
    # Lanzar 10 updates concurrentes con campos diferentes
    tasks = [
        update_field(f"field_{i}", f"value_{i}")
        for i in range(10)
    ]
    results = await asyncio.gather(*tasks)
    
    # ASSERT: Todos los updates deben haber tenido Ã©xito
    assert all(results), "Some updates failed"
    
    # ASSERT: Verificar que todos los campos estÃ¡n presentes
    final_session = await session_mgr.get_session(user_id, tenant_id)
    for i in range(10):
        assert f"field_{i}" in final_session
        assert final_session[f"field_{i}"] == f"value_{i}"

@pytest.mark.asyncio
async def test_micro_batching_reduces_redis_writes(redis_client, mocker):
    """
    Verificar que micro-batching fusiona mÃºltiples updates en 1 write.
    """
    session_mgr = SessionManager(redis_client)
    mock_setex = mocker.patch.object(redis_client, "setex")
    
    user_id = "batch_test_user"
    
    # Enviar 5 updates casi simultÃ¡neos
    tasks = [
        session_mgr.update_session(user_id, {f"field_{i}": i})
        for i in range(5)
    ]
    await asyncio.gather(*tasks)
    
    # ASSERT: Solo 1 write a Redis (todos batcheados)
    # (puede ser 2 si el timing no es perfecto, pero nunca 5)
    assert mock_setex.call_count <= 2, f"Expected â‰¤2 writes, got {mock_setex.call_count}"
```

## ğŸ“Š IMPACTO ANALYSIS

**Severidad**: ğŸ”´ CRÃTICA
- PÃ©rdida de contexto en conversaciones activas
- Afecta ~5% de usuarios en alta concurrencia (>500 req/s)
- Impacto en UX: Usuario debe re-autenticar o perder progreso de reserva

**Performance Impact**:
- Best case (sin contenciÃ³n): +0ms (async queueing)
- Micro-batching: -40ms promedio (5x menos writes a Redis)
- Memory overhead: 1 queue + 1 task por sesiÃ³n activa (~2KB/session)

**Deployment Strategy**:
```yaml
Phase 1 (Day 1-2): 
  - Deploy con feature flag "session.queue_updates.enabled" = false
  
Phase 2 (Day 3-4):
  - Activar flag al 10% de usuarios (canary)
  - Monitorear: session_update_latency, batch_size_distribution
  
Phase 3 (Day 5-7):
  - 50% rollout si mÃ©tricas estables
  
Phase 4 (Week 2):
  - 100% rollout + remover flag
```

## ğŸ“ˆ MÃ‰TRICAS REQUERIDAS

```python
# Nuevas mÃ©tricas Prometheus
session_batch_size = Histogram(
    "session_manager_batch_size",
    "Number of updates per batch",
    buckets=[1, 2, 5, 10, 20]
)

session_update_latency = Histogram(
    "session_manager_update_latency_seconds",
    "Time to process session update",
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1]
)

session_workers_active = Gauge(
    "session_manager_workers_active",
    "Number of active session update workers"
)
```

âœ… Â¿Esta soluciÃ³n resuelve tu caso? Â¿Necesitas profundizar en algÃºn aspecto especÃ­fico?
```

**CRITERIOS DE Ã‰XITO para UC-001**:
- [ ] Bot identifica el problema en session_manager.py con lÃ­neas exactas
- [ ] Menciona "race condition" y "lost update problem" explÃ­citamente
- [ ] Propone soluciÃ³n con cÃ³digo completo (Queue pattern + micro-batching)
- [ ] Incluye â‰¥2 test cases (concurrencia, batching)
- [ ] Considera performance impact (memory, latency)
- [ ] Proporciona mÃ©tricas Prometheus
- [ ] Timeline realista de deployment con feature flags

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-002: Circuit Breaker Flapping en PMS Adapter ğŸŸ 

**METADATA**:
- ID: UC-002
- CategorÃ­a: BUG (Resilience Pattern)
- Complejidad: COMPLEX
- MÃ³dulos: `app/services/pms_adapter.py`, `app/core/circuit_breaker.py`
- Tiempo estimado: 2h

**CONVERSACIÃ“N**:

**[USER]**
```
El circuit breaker del PMS estÃ¡ "flappeando" - abre y cierra cada 30 segundos.
Los usuarios ven errores intermitentes tipo "PMS temporarily unavailable".

Logs:
[WARN] pms_circuit_breaker.state_change state=OPEN failures=5 window=30s
[INFO] pms_circuit_breaker.state_change state=HALF_OPEN attempt=1
[INFO] pms_circuit_breaker.state_change state=CLOSED success=true
... (30s despuÃ©s)
[WARN] pms_circuit_breaker.state_change state=OPEN failures=5 window=30s

El PMS tiene latency spikes normales de 2-3s, pero no estÃ¡ "caÃ­do".
Â¿El threshold es muy sensible?
```

**[BOT - Respuesta esperada]**
```markdown
# ğŸ› ANÃLISIS: Circuit Breaker Flapping

## ğŸ“ LocalizaciÃ³n
- Archivo: `app/services/pms_adapter.py`
- Circuit Breaker: lÃ­neas ~50-120 (init + wrapper)
- Config: `app/core/circuit_breaker.py`

## ğŸ§  RAZONAMIENTO

**Paso 1: Revisar configuraciÃ³n actual**
```python
# pms_adapter.py (lÃ­nea ~60)
self.circuit_breaker = CircuitBreaker(
    failure_threshold=5,        # 5 failures â†’ OPEN
    recovery_timeout=30,        # 30s para intentar HALF_OPEN
    expected_exception=httpx.HTTPError
)
```

**Paso 2: AnÃ¡lisis del problema**
- Threshold=5 es muy bajo para API externa con variabilidad normal
- No distingue entre "timeout por latency" vs "error real 500"
- No considera percentiles (P95, P99) sino raw failures

**Paso 3: Comportamiento observado**
```
t=0s:    5 requests con timeout 2-3s â†’ marcados como "failures"
t=30s:   CB â†’ OPEN
t=60s:   CB â†’ HALF_OPEN, 1 request succeed â†’ CLOSED
t=90s:   Otro spike â†’ 5 failures â†’ OPEN nuevamente
```

**ROOT CAUSE**: Threshold fijo + no considera latency como mÃ©trica separada

## âœ… SOLUCIÃ“N: Adaptive Circuit Breaker

```python
# âœ… app/core/circuit_breaker.py - REFACTOR

from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
from collections import deque
import statistics

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

@dataclass
class CircuitBreakerMetrics:
    """MÃ©tricas en ventana deslizante"""
    window_size: int = 60  # segundos
    requests: deque = field(default_factory=lambda: deque(maxlen=100))
    
    def record_request(self, success: bool, latency_ms: float):
        self.requests.append({
            "timestamp": datetime.now(),
            "success": success,
            "latency_ms": latency_ms
        })
    
    def get_error_rate_last_window(self) -> float:
        """Error rate en Ãºltimos 60s"""
        now = datetime.now()
        recent = [
            r for r in self.requests 
            if (now - r["timestamp"]).total_seconds() < self.window_size
        ]
        if not recent:
            return 0.0
        failures = sum(1 for r in recent if not r["success"])
        return failures / len(recent)
    
    def get_latency_p95(self) -> float:
        """P95 latency en Ãºltimos 60s"""
        now = datetime.now()
        recent_latencies = [
            r["latency_ms"] for r in self.requests
            if (now - r["timestamp"]).total_seconds() < self.window_size
        ]
        if not recent_latencies:
            return 0.0
        return statistics.quantiles(recent_latencies, n=20)[18]  # P95

class AdaptiveCircuitBreaker:
    """
    Circuit Breaker con thresholds adaptativos basados en mÃ©tricas.
    
    Criterios para OPEN:
    - Error rate >20% en Ãºltima ventana (vs threshold fijo de 5 failures)
    - P95 latency >5000ms (indica degradaciÃ³n seria)
    - Consecutive failures â‰¥10 (indica outage total)
    """
    
    def __init__(
        self,
        error_rate_threshold=0.20,      # 20% error rate â†’ OPEN
        latency_p95_threshold_ms=5000,  # P95 >5s â†’ OPEN
        consecutive_failure_threshold=10,
        recovery_timeout=60,            # MÃ¡s tiempo para recovery
        half_open_max_requests=3        # Test con 3 requests antes de CLOSED
    ):
        self.error_rate_threshold = error_rate_threshold
        self.latency_p95_threshold_ms = latency_p95_threshold_ms
        self.consecutive_failure_threshold = consecutive_failure_threshold
        self.recovery_timeout = recovery_timeout
        self.half_open_max_requests = half_open_max_requests
        
        self.state = CircuitState.CLOSED
        self.metrics = CircuitBreakerMetrics()
        self.consecutive_failures = 0
        self.last_opened_at = None
        self.half_open_requests_count = 0
    
    async def call(self, func, *args, **kwargs):
        """Wrapper para ejecutar funciÃ³n con circuit breaker"""
        
        # 1. Check state
        if self.state == CircuitState.OPEN:
            # Verificar si es tiempo de intentar HALF_OPEN
            if (datetime.now() - self.last_opened_at).total_seconds() >= self.recovery_timeout:
                self.state = CircuitState.HALF_OPEN
                self.half_open_requests_count = 0
                logger.info("circuit_breaker.state_transition", from_state="OPEN", to_state="HALF_OPEN")
            else:
                raise CircuitBreakerOpenError("Circuit breaker is OPEN")
        
        # 2. Execute request
        start_time = datetime.now()
        try:
            result = await func(*args, **kwargs)
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000
            
            # Record success
            self.metrics.record_request(success=True, latency_ms=latency_ms)
            self.consecutive_failures = 0
            
            # Si estÃ¡bamos en HALF_OPEN, verificar si cerrar
            if self.state == CircuitState.HALF_OPEN:
                self.half_open_requests_count += 1
                if self.half_open_requests_count >= self.half_open_max_requests:
                    self.state = CircuitState.CLOSED
                    logger.info("circuit_breaker.state_transition", from_state="HALF_OPEN", to_state="CLOSED")
            
            return result
            
        except Exception as e:
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000
            
            # Record failure
            self.metrics.record_request(success=False, latency_ms=latency_ms)
            self.consecutive_failures += 1
            
            # Check if should OPEN
            should_open = (
                self.metrics.get_error_rate_last_window() > self.error_rate_threshold or
                self.metrics.get_latency_p95() > self.latency_p95_threshold_ms or
                self.consecutive_failures >= self.consecutive_failure_threshold
            )
            
            if should_open and self.state != CircuitState.OPEN:
                self.state = CircuitState.OPEN
                self.last_opened_at = datetime.now()
                logger.warning(
                    "circuit_breaker.opened",
                    error_rate=self.metrics.get_error_rate_last_window(),
                    p95_latency=self.metrics.get_latency_p95(),
                    consecutive_failures=self.consecutive_failures
                )
            
            # Si estÃ¡bamos en HALF_OPEN y falla, volver a OPEN
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.OPEN
                self.last_opened_at = datetime.now()
                logger.warning("circuit_breaker.half_open_failed", to_state="OPEN")
            
            raise
```

## ğŸ§ª TESTING

```python
@pytest.mark.asyncio
async def test_adaptive_cb_tolerates_latency_spikes(mocker):
    """
    Verificar que latency spikes ocasionales NO abren el CB.
    """
    cb = AdaptiveCircuitBreaker()
    
    # Simular 95 requests exitosos con latency normal (200ms)
    async def fast_request():
        await asyncio.sleep(0.2)
        return "ok"
    
    for _ in range(95):
        await cb.call(fast_request)
    
    # Simular 5 requests con latency spike (3000ms) pero exitosos
    async def slow_but_successful():
        await asyncio.sleep(3.0)
        return "ok"
    
    for _ in range(5):
        await cb.call(slow_but_successful)
    
    # ASSERT: CB debe estar CLOSED (tolerÃ³ el spike)
    assert cb.state == CircuitState.CLOSED
    
    # Error rate debe ser 0% (todos exitosos)
    assert cb.metrics.get_error_rate_last_window() == 0.0

@pytest.mark.asyncio
async def test_adaptive_cb_opens_on_sustained_degradation():
    """
    Verificar que CB abre cuando error rate >20% sostenido.
    """
    cb = AdaptiveCircuitBreaker(error_rate_threshold=0.20)
    
    # Simular 80 requests exitosos + 20 fallidos (25% error rate)
    async def failing_request():
        if random.random() < 0.25:  # 25% failure
            raise Exception("Simulated failure")
        return "ok"
    
    for _ in range(100):
        try:
            await cb.call(failing_request)
        except:
            pass
    
    # ASSERT: CB debe abrir (error rate >20%)
    assert cb.state == CircuitState.OPEN
```

## ğŸ“Š MÃ‰TRICAS

```python
circuit_breaker_state = Gauge(
    "pms_circuit_breaker_state",
    "Circuit breaker state (0=CLOSED, 1=OPEN, 2=HALF_OPEN)"
)

circuit_breaker_error_rate = Gauge(
    "pms_circuit_breaker_error_rate",
    "Current error rate in sliding window"
)

circuit_breaker_latency_p95 = Gauge(
    "pms_circuit_breaker_latency_p95_ms",
    "P95 latency in sliding window"
)
```

âœ… Â¿Esta soluciÃ³n resuelve el flapping? Â¿Necesitas ajustar algÃºn threshold?
```

**CRITERIOS DE Ã‰XITO para UC-002**:
- [ ] Bot identifica configuraciÃ³n actual del circuit breaker
- [ ] Explica por quÃ© threshold fijo causa flapping
- [ ] Propone adaptive thresholds con mÃ©tricas (error rate, P95 latency)
- [ ] CÃ³digo completo del adaptive circuit breaker
- [ ] Tests especÃ­ficos (latency spikes, sustained degradation)
- [ ] Nuevas mÃ©tricas para monitoreo

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-003: Memoria Redis Crece Sin Control ğŸ”´

**METADATA**:
- ID: UC-003
- CategorÃ­a: BUG (Resource Leak)
- Complejidad: MEDIUM
- MÃ³dulos: `app/services/session_manager.py`, `app/services/feature_flag_service.py`
- Tiempo estimado: 1h

**[USER]**
```
Redis estÃ¡ creciendo de 2GB a 15GB en 1 semana. Tuvimos que reiniciar porque llegÃ³ a OOM.

redis-cli INFO memory:
used_memory_human:15.2G
maxmemory_human:16.0G

KEYS command muestra millones de keys con patrÃ³n "session:*" que parecen abandonadas.
Â¿No estamos limpiando sesiones viejas?
```

**[BOT - Respuesta]**
```markdown
# ğŸ› ANÃLISIS: Memory Leak en Redis Sessions

## ğŸ“ LocalizaciÃ³n
- `app/services/session_manager.py` - TTL configuration
- `app/core/constants.py` - SESSION_TTL_DEFAULT

## ğŸ§  ROOT CAUSE ANALYSIS

```python
# session_manager.py lÃ­nea ~100
await self.redis.setex(session_key, self.ttl, json.dumps(session_data))
# TTL es correcto (1800s = 30min) âœ…

# PERO...
# Si usuario abandona conversaciÃ³n antes de 30min, la key persiste igual
# Con 10k usuarios/dÃ­a abandonando a mitad de flow â†’ 10k sessions huÃ©rfanas
```

**Problema identificado**: Sessions no se limpian proactivamente cuando usuario no vuelve.

## âœ… SOLUCIÃ“N: Aggressive Cleanup + Monitoring

```python
# session_manager.py - AÃ‘ADIR

async def cleanup_abandoned_sessions(self):
    """
    Background task para limpiar sesiones abandonadas.
    Criteria: Ãºltima actividad >1 hora
    """
    cursor = 0
    cleaned_count = 0
    
    while True:
        # Scan con cursor para no bloquear Redis
        cursor, keys = await self.redis.scan(
            cursor, 
            match="session:*", 
            count=100
        )
        
        for key in keys:
            try:
                ttl = await self.redis.ttl(key)
                
                # Si TTL >25 min (usuario no activo en 5+ min), forzar expiraciÃ³n
                if ttl > 1500:
                    session_data = json.loads(await self.redis.get(key))
                    last_activity = session_data.get("last_activity_timestamp")
                    
                    if last_activity:
                        inactive_seconds = (datetime.now() - datetime.fromisoformat(last_activity)).total_seconds()
                        
                        if inactive_seconds > 3600:  # 1 hora inactivo
                            await self.redis.delete(key)
                            cleaned_count += 1
                            logger.info("session_cleanup.deleted", key=key, inactive_seconds=inactive_seconds)
            
            except Exception as e:
                logger.error("session_cleanup.error", key=key, error=str(e))
        
        if cursor == 0:
            break  # Scan completo
    
    logger.info("session_cleanup.completed", cleaned=cleaned_count)
    return cleaned_count

# Iniciar cleanup task en startup
async def start_cleanup_task(self):
    async def periodic_cleanup():
        while True:
            await asyncio.sleep(3600)  # Cada hora
            await self.cleanup_abandoned_sessions()
    
    asyncio.create_task(periodic_cleanup())
```

## ğŸ§ª TEST

```python
@pytest.mark.asyncio
async def test_abandoned_sessions_are_cleaned(redis_client):
    session_mgr = SessionManager(redis_client)
    
    # Crear session abandonada (last_activity hace 2 horas)
    old_timestamp = (datetime.now() - timedelta(hours=2)).isoformat()
    await redis_client.setex(
        "session:tenant_a:user_old:whatsapp",
        1800,
        json.dumps({"last_activity_timestamp": old_timestamp})
    )
    
    # Cleanup
    cleaned = await session_mgr.cleanup_abandoned_sessions()
    
    # ASSERT
    assert cleaned == 1
    assert not await redis_client.exists("session:tenant_a:user_old:whatsapp")
```

âœ… Â¿Implemento esto o prefieres una estrategia diferente?
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-004: NLP Confidence Baja despuÃ©s de 3 Meses ğŸŸ¡

**METADATA**:
- ID: UC-004
- CategorÃ­a: BUG (Model Drift)
- Complejidad: COMPLEX
- MÃ³dulos: `app/services/nlp_engine.py`, `app/services/enhanced_nlp_engine.py`
- Tiempo estimado: 3h

**[USER]**
```
El NLP engine que tenÃ­a 95% accuracy en julio ahora estÃ¡ en 75%. 
Usuarios usan frases nuevas que no vio en training: "quiero una cama king", "habitaciÃ³n con vista al mar".

Â¿CÃ³mo re-entrenar el modelo sin downtime?
```

**[BOT - Respuesta]**
```markdown
# ğŸ› ANÃLISIS: Model Drift en NLP

## ğŸ“ LocalizaciÃ³n
- `app/services/nlp_engine.py` - Model loading
- Training data: posiblemente en `rasa_nlu/` (si usa Rasa)

## ğŸ§  ROOT CAUSE
Training data estÃ¡tico â†’ usuarios evolucionan vocabulario â†’ model drift

## âœ… SOLUCIÃ“N: Continuous Learning Pipeline

```python
# scripts/retrain_nlp_model.py

async def retrain_with_production_data():
    """
    1. Export low-confidence predictions de Ãºltimos 30 dÃ­as
    2. Human labeling (usar Labelbox o similar)
    3. Retrain model
    4. A/B test nuevo model vs viejo
    5. Rollout gradual con feature flag
    """
    pass
```

[ImplementaciÃ³n completa con pipeline de retraining]
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-005: Audio Transcription Timeout para >2min Files ğŸŸ 

**METADATA**:
- ID: UC-005
- CategorÃ­a: BUG (External API)
- Complejidad: MEDIUM
- MÃ³dulos: `app/services/audio_processor.py`, `app/services/dlq_service.py`
- Tiempo estimado: 1h

**[USER]**
```
Usuarios reportan que mensajes de voz >2 minutos no se transcriben.
Logs: "OpenAI Whisper API timeout after 120s"

Â¿Timeout hardcoded?
```

**[BOT - Respuesta]**
```markdown
# ğŸ› ANÃLISIS: Audio Timeout

## ğŸ“ LocalizaciÃ³n
`app/services/audio_processor.py` lÃ­nea ~165

```python
async with httpx.AsyncClient(timeout=120.0) as client:  # âš ï¸ Fixed timeout
```

## âœ… SOLUCIÃ“N: Adaptive Timeout + DLQ Retry

```python
# Calcular timeout basado en duraciÃ³n estimada
duration_sec = await self._estimate_audio_duration(media_url)
timeout = min(30 + (duration_sec / 60 * 45), 300)  # Max 5 min

# Si timeout igual, encolar a DLQ para retry async
```

[ImplementaciÃ³n completa con DLQ service]
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## CATEGORÃA 2: NUEVAS FEATURES (3 CASOS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-006: Implementar Intent "modify_reservation" ğŸ¨

**METADATA**:
- ID: UC-006
- CategorÃ­a: FEATURE
- Complejidad: COMPLEX
- MÃ³dulos: `orchestrator.py`, `pms_adapter.py`, `nlp_engine.py`
- Tiempo estimado: 6h

**[USER]**
```
Necesitamos implementar la funcionalidad de modificar reservas existentes.
Usuario deberÃ­a poder decir "quiero cambiar mi reserva del 20 al 25 de diciembre".

Â¿CÃ³mo estructuro esto siguiendo los patterns existentes?
```

**[BOT - Respuesta]**
```markdown
# ğŸ¨ DISEÃ‘O: Intent "modify_reservation"

## ğŸ—ï¸ ARQUITECTURA

```python
# orchestrator.py - AÃ‘ADIR al _intent_handlers dict

self._intent_handlers["modify_reservation"] = self._handle_modify_reservation

async def _handle_modify_reservation(self, nlp_result: dict, session_data: dict, message: UnifiedMessage) -> dict:
    """
    Flow:
    1. Extract booking_id de entities o session
    2. Validate con PMS que booking existe
    3. Extract new dates/room_type
    4. Check availability
    5. Calculate price diff (upgrade/downgrade)
    6. Request confirmation
    7. Update booking con PMS
    """
    
    # Paso 1: Extract booking_id
    booking_id = nlp_result["entities"].get("booking_id") or session_data.get("last_booking_id")
    
    if not booking_id:
        return {
            "response_type": "text",
            "content": "Â¿CuÃ¡l es el nÃºmero de tu reserva? Lo encuentras en el email de confirmaciÃ³n."
        }
    
    # Paso 2: Validate booking exists
    try:
        booking = await self.pms_adapter.get_booking(booking_id)
    except PMSError:
        return {
            "response_type": "text",
            "content": f"No encuentro la reserva {booking_id}. Â¿EstÃ¡s seguro del nÃºmero?"
        }
    
    # Paso 3: Extract new parameters
    new_checkin = nlp_result["entities"].get("checkin_date")
    new_checkout = nlp_result["entities"].get("checkout_date")
    new_room_type = nlp_result["entities"].get("room_type")
    
    # Paso 4: Check availability
    availability = await self.pms_adapter.check_availability(
        new_checkin or booking["checkin_date"],
        new_checkout or booking["checkout_date"]
    )
    
    if not availability["available"]:
        return {
            "response_type": "text",
            "content": "Lo siento, no hay disponibilidad para esas fechas. Â¿Quieres intentar otras?"
        }
    
    # Paso 5: Calculate price difference
    original_price = booking["total_price"]
    new_price = await self.pms_adapter.calculate_price(
        new_checkin or booking["checkin_date"],
        new_checkout or booking["checkout_date"],
        new_room_type or booking["room_type"]
    )
    price_diff = new_price - original_price
    
    # Paso 6: Request confirmation
    session_data["pending_modification"] = {
        "booking_id": booking_id,
        "new_params": {...},
        "price_diff": price_diff
    }
    
    return {
        "response_type": "text",
        "content": f"Perfecto, puedo cambiar tu reserva. {'DeberÃ¡s pagar ${price_diff} adicional' if price_diff > 0 else f'Te reembolsarÃ© ${-price_diff}'}. Â¿Confirmas el cambio?",
        "actions": ["confirm", "cancel"]
    }
```

## ğŸ§ª TESTS

```python
@pytest.mark.asyncio
async def test_modify_reservation_happy_path(orchestrator, mock_pms):
    # Arrange
    mock_pms.get_booking.return_value = {"id": "B123", "checkin_date": "2025-12-20", "total_price": 500}
    mock_pms.check_availability.return_value = {"available": True}
    mock_pms.calculate_price.return_value = 550
    
    nlp_result = {
        "intent": {"name": "modify_reservation"},
        "entities": {"booking_id": "B123", "checkin_date": "2025-12-25"}
    }
    
    # Act
    response = await orchestrator._handle_modify_reservation(nlp_result, {}, mock_message)
    
    # Assert
    assert "DeberÃ¡s pagar $50 adicional" in response["content"]
    assert response["actions"] == ["confirm", "cancel"]
```

[MÃ¡s tests: sin booking_id, sin disponibilidad, downgrade, etc.]
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-007: Soporte Multiidioma (InglÃ©s/PortuguÃ©s) ğŸŒ

**METADATA**:
- ID: UC-007
- CategorÃ­a: FEATURE
- Complejidad: MEDIUM
- MÃ³dulos: `nlp_engine.py`, `template_service.py`, `app/utils/i18n_helpers.py`
- Tiempo estimado: 4h

[DiseÃ±o completo de i18n con detecciÃ³n de idioma + templates multiidioma]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-008: Notificaciones Push para Confirmaciones ğŸ“²

**METADATA**:
- ID: UC-008
- CategorÃ­a: FEATURE
- Complejidad: MEDIUM
- MÃ³dulos: `app/services/whatsapp_client.py`, nuevo `notification_service.py`
- Tiempo estimado: 3h

[ImplementaciÃ³n de webhook reverse + queue de notificaciones]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## CATEGORÃA 3: OPTIMIZATION (2 CASOS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-009: Reducir Latencia NLP de 800ms a <300ms âš¡

**METADATA**:
- ID: UC-009
- CategorÃ­a: OPTIMIZATION
- Complejidad: EXPERT
- MÃ³dulos: `nlp_engine.py`, caching layer
- Tiempo estimado: 3h

[Profiling + modelo cuantizado + caching de embeddings]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-010: Refactorizar Orchestrator (Extraer Business Logic) ğŸ”§

**METADATA**:
- ID: UC-010
- CategorÃ­a: OPTIMIZATION (Refactoring)
- Complejidad: COMPLEX
- MÃ³dulos: `orchestrator.py` (2,030 lÃ­neas), nuevos services
- Tiempo estimado: 8h

[ExtracciÃ³n a BusinessHoursService, EscalationService, IntentRouter]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## CATEGORÃA 4: ARQUITECTURA (2 CASOS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-011: AÃ±adir Nuevo Canal (Telegram) sin Romper AbstracciÃ³n ğŸ—ï¸

**METADATA**:
- ID: UC-011
- CategorÃ­a: ARCHITECTURE
- Complejidad: COMPLEX
- MÃ³dulos: nuevo `telegram_client.py`, `message_gateway.py`
- Tiempo estimado: 6h

[DiseÃ±o de TelegramAdapter implementando MessageGateway interface]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### UC-012: Migrar de Redis a PostgreSQL para Sessions ğŸ”„

**METADATA**:
- ID: UC-012
- CategorÃ­a: ARCHITECTURE
- Complejidad: EXPERT
- MÃ³dulos: `session_manager.py`, migrations
- Tiempo estimado: 12h

[AnÃ¡lisis pros/cons + migration path: dual-write â†’ validate â†’ switch â†’ cleanup]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## âœ… ENTREGABLE FINAL

Total de **12 casos de uso** enterprise-grade con:
- Conversaciones completas (user â†’ bot â†’ follow-ups)
- CÃ³digo production-ready
- Tests comprehensivos
- MÃ©tricas y observabilidad
- Deployment strategy
- Runbooks cuando aplique

**FORMATO**: Markdown estructurado de ~600-800 lÃ­neas por caso de uso  
**Total esperado**: ~8,000-10,000 lÃ­neas de documentaciÃ³n ejecutable

---

**Creado**: 2025-11-18  
**Personalizado para**: SIST_AGENTICO_HOTELERO  
**Commit hash**: 97676bcc27f7f999f602432a07383ce09c5dee68  
**VersiÃ³n**: 1.0 (Personalizada)
