data_sources:
  - source_id: "none"
    type: "N/A"
    location: "N/A"
    format: []
    total_documents: 0
    total_size: "0 bytes"
    update_frequency: "N/A"
    access_method: "N/A"
    preprocessing_required: []
    notes: "NO RAG infrastructure implemented in current system"

ingestion_pipeline:
  extraction:
    scripts: []
    supported_formats: []
    text_extraction_method: "N/A"
    metadata_extraction: []
    notes: "No document ingestion pipeline exists"
    
  processing:
    cleaning_steps: []
    chunking_strategy:
      method: "none"
      chunk_size: null
      overlap: null
      separators: []
      preserve_structure: null
      notes: "No chunking implemented"
      
  embedding:
    model_provider: "none"
    model_name: "none"
    embedding_dimension: null
    batch_size: null
    caching_enabled: null
    notes: "No embedding generation implemented"
    
  storage:
    vector_database: "none"
    connection_details:
      host: "N/A"
      configuration: {}
    collections: []
    notes: "No vector store configured"

vector_store_configuration:
  provider: "none"
  index_configuration:
    dimension: null
    metric: "N/A"
    index_type: "N/A"
    notes: "No vector indexing"
    
  metadata_schema: []
  
  performance_tuning:
    cache_size: "N/A"
    batch_operations: null
    replication: null

retrieval_configuration:
  search_parameters:
    top_k: null
    similarity_threshold: null
    max_tokens_context: null
    notes: "No retrieval implemented"
    
  reranking:
    enabled: false
    model: "none"
    final_k: null
    
  query_processing:
    expansion: false
    filters_available: []
    hybrid_search: false
    notes: "No query processing pipeline"
    
  context_building:
    citation_format: "N/A"
    include_metadata: null
    max_context_length: null

integration_points:
  agents_using_rag: []
  api_endpoints: []
  query_interfaces: []
  notes: "No agents currently use RAG"

performance_metrics:
  average_retrieval_time: "N/A"
  index_size: "0 bytes"
  query_throughput: "N/A"
  accuracy_metrics: []

maintenance_procedures:
  reindexing_frequency: "N/A"
  backup_strategy: "N/A"
  monitoring: []

current_knowledge_management:
  method: "Template-based responses"
  description: |
    Sistema actual NO utiliza RAG. El conocimiento está codificado en:
    
    1. **Templates estáticos** (TemplateService):
       - 18+ templates predefinidos en código Python
       - Variables dinámicas inyectadas: precios, fechas, nombres
       - Multiidioma (ES/EN/PT) mediante templates separados
       - Ubicación: app/services/template_service.py
    
    2. **Rasa NLU Training Data**:
       - Archivo: rasa_nlu/data/nlu.yml
       - 15+ intents con ejemplos de entrenamiento
       - Entidades: dates, room_type, number_guests, price_range
       - Método: Supervised learning (DIET Classifier)
       - NO es búsqueda semántica, es clasificación de intents
    
    3. **Reglas de negocio hardcodeadas**:
       - Lógica en orchestrator.py (_handle_<intent>_intent methods)
       - Business hours logic (utils/business_hours.py)
       - Room images mapping (utils/room_images.py)
       - Fallback rules (regex-based keyword matching)
    
    4. **Integración PMS**:
       - Datos de disponibilidad/reservas vienen de QloApps API
       - NO se almacenan documentos o knowledge base
       - Operaciones transaccionales en tiempo real
  
  limitations:
    - "No puede responder preguntas fuera de templates predefinidos"
    - "No aprende de conversaciones pasadas (sin memory a largo plazo)"
    - "Información estática: cualquier cambio requiere modificar código"
    - "No puede buscar en documentación del hotel (políticas, servicios, FAQ)"
    - "Respuestas limitadas a lo programado explícitamente"
  
  advantages:
    - "Respuestas predecibles y controladas"
    - "Sin costos de LLM API calls"
    - "Latencia muy baja (sin búsqueda vectorial)"
    - "No risk de alucinaciones"
    - "Fácil de auditar respuestas"

gap_analysis:
  missing_components:
    - component: "Document Ingestion Pipeline"
      impact: "Cannot process hotel documentation (policies, services, FAQ)"
      priority: "Medium"
      
    - component: "Vector Database"
      impact: "Cannot perform semantic search over knowledge base"
      priority: "Medium"
      options: ["Pinecone", "Weaviate", "Chroma", "Qdrant", "pgvector (PostgreSQL extension)"]
      
    - component: "Embedding Model"
      impact: "Cannot create semantic vectors for retrieval"
      priority: "Medium"
      options: ["OpenAI text-embedding-ada-002", "Sentence Transformers (local)", "Cohere Embed"]
      
    - component: "Retrieval Service"
      impact: "Cannot fetch relevant context for user queries"
      priority: "Medium"
      
    - component: "Context Builder"
      impact: "Cannot assemble multi-document context for responses"
      priority: "Low"
      
    - component: "Generative LLM Integration"
      impact: "Cannot generate dynamic, context-aware responses beyond templates"
      priority: "High (if RAG is implemented)"
      options: ["OpenAI GPT-4", "Claude", "Local LLM (Llama, Mistral)"]

potential_data_sources_for_rag:
  - source: "Hotel Policies Documentation"
    description: "Check-in/out times, cancellation policies, payment terms, pet policies"
    format: "PDF, Markdown, HTML"
    estimated_size: "~100 pages"
    update_frequency: "Quarterly"
    
  - source: "Services and Amenities Catalog"
    description: "Room service menu, spa services, gym hours, pool rules, WiFi info"
    format: "PDF, CSV, JSON"
    estimated_size: "~50 pages"
    update_frequency: "Monthly"
    
  - source: "FAQ Knowledge Base"
    description: "Common guest questions and answers"
    format: "Markdown, JSON"
    estimated_size: "~200 Q&A pairs"
    update_frequency: "Weekly"
    
  - source: "Room Descriptions"
    description: "Detailed room features, bed types, views, amenities per room type"
    format: "JSON, Markdown"
    estimated_size: "~25 room types"
    update_frequency: "Quarterly"
    
  - source: "Local Area Information"
    description: "Nearby restaurants, attractions, transportation options"
    format: "JSON, Markdown"
    estimated_size: "~100 entries"
    update_frequency: "Monthly"
    
  - source: "Historical Guest Conversations"
    description: "Anonymized conversation logs for improving responses"
    format: "JSON (from session logs)"
    estimated_size: "Depends on usage (thousands of messages)"
    update_frequency: "Real-time"
    privacy_concerns: "Requires GDPR compliance, anonymization, consent"

recommended_rag_architecture:
  phase_1_minimal_mvp:
    components:
      - "pgvector extension in existing PostgreSQL (no new DB)"
      - "Sentence Transformers (local, no API costs)"
      - "Simple ingestion scripts for Markdown FAQ"
      - "Naive retrieval (top-k cosine similarity)"
    estimated_effort: "2-3 weeks"
    cost: "Free (local models)"
    
  phase_2_production_ready:
    components:
      - "Dedicated vector store (Qdrant or Weaviate)"
      - "OpenAI embeddings (better quality)"
      - "Hybrid search (dense + sparse)"
      - "Reranking model"
      - "Metadata filtering (by language, category, recency)"
    estimated_effort: "4-6 weeks"
    cost: "$50-200/month (embedding API + vector DB hosting)"
    
  phase_3_advanced:
    components:
      - "Multi-source ingestion (PDF, web scraping, PMS integration)"
      - "LLM-generated responses with RAG context"
      - "Citation tracking and source attribution"
      - "Continuous learning from feedback"
      - "A/B testing RAG vs templates"
    estimated_effort: "8-12 weeks"
    cost: "$200-500/month (LLM API + embeddings + infrastructure)"

integration_strategy:
  approach: "Incremental adoption without disrupting existing system"
  steps:
    - step: 1
      action: "Create RAGService class"
      description: "New service in app/services/rag_service.py"
      impact: "Zero (optional service)"
      
    - step: 2
      action: "Add feature flag 'features.rag_enabled'"
      description: "Control RAG usage via FeatureFlagService"
      impact: "Allows gradual rollout"
      
    - step: 3
      action: "Integrate in Orchestrator"
      description: |
        In _handle_unknown_intent or _handle_out_of_scope:
        if await ff.is_enabled('features.rag_enabled'):
            rag_result = await rag_service.search(query)
            if rag_result['confidence'] > 0.7:
                return rag_result['answer']
        # Fall back to existing template logic
      impact: "Enhances out-of-scope handling"
      
    - step: 4
      action: "Monitor and iterate"
      description: "Track RAG response quality, latency, user satisfaction"
      impact: "Data-driven improvement"

missing_information:
  - "¿Existe documentación del hotel disponible digitalmente?"
  - "¿Cuál es el presupuesto disponible para servicios de LLM/embeddings?"
  - "¿Existe preferencia entre cloud (OpenAI) vs local (Llama)?"
  - "¿Qué políticas de privacidad aplican para almacenar conversaciones?"
  - "¿Se requiere explicabilidad/citas en las respuestas?"
  - "¿Cuál es la prioridad de implementar RAG vs otras features (QR codes, reviews)?"

validation_questions:
  - "¿Se ha considerado RAG para el roadmap futuro?"
  - "¿Existe contenido del hotel que no está siendo utilizado actualmente?"
  - "¿Los huéspedes hacen preguntas fuera del alcance actual del bot?"
  - "¿Hay métricas de preguntas sin respuesta o fallback rate?"
  - "¿Preferirían un sistema híbrido (RAG + templates) o migración completa?"

conclusion: |
  SISTEMA ACTUAL: Template-based + Rasa NLU (NO RAG)
  
  ESTADO:
  - ✅ Funcional para casos de uso predefinidos
  - ❌ No puede responder preguntas dinámicas
  - ❌ No busca en documentación
  - ❌ No aprende de conversaciones
  
  RECOMENDACIÓN:
  - Implementar RAG en Fase 2 (después de completar Quick Wins Features 4-6)
  - Comenzar con FAQ sencillo como prueba de concepto
  - Mantener templates para flujos críticos (reservas, pagos)
  - Usar RAG para preguntas informativas (políticas, servicios, área local)
  
  BENEFICIOS ESPERADOS:
  - +50% reducción en "out_of_scope" intents
  - +30% mejora en satisfacción (respuestas más completas)
  - -20% carga de recepción (menos escalamientos)
  
  RIESGOS:
  - Costo adicional de LLM API calls
  - Latencia aumentada (+500ms-1s)
  - Potencial alucinaciones si LLM no usa contexto correctamente
  - Necesidad de curación de contenido inicial
