# [PROMPT 3.6] Manual de Operaciones - Agente Hotel

## üìã √çndice

1. [Operaci√≥n Diaria](#operaci√≥n-diaria)
2. [Troubleshooting](#troubleshooting)
3. [Runbooks](#runbooks)
4. [Mantenimiento](#mantenimiento)
5. [Recuperaci√≥n de Desastres](#recuperaci√≥n-de-desastres)

---

## Operaci√≥n Diaria

### Checklist Matutino (08:00 - 5 minutos)

- **Verificar salud:** `curl https://tu-hotel.com.ar/health/ready`
- **Revisar logs de errores:** `docker logs agente-api --since="8h" | grep ERROR`
- **Verificar backups:** `ls -la /backups/agente-hotel/daily/`

### Par√°metros SLO

- `SLO_TARGET` (default 99.0) controla el objetivo global de √©xito del orquestador.
- El error budget = 1 - (SLO_TARGET/100) se inserta din√°micamente en las recording rules al iniciar Prometheus.
- Para cambiarlo: editar `.env`, reiniciar servicio `prometheus` y validar reglas regeneradas.
- Pisos de tr√°fico: las alertas SLO requieren `orchestrator_message_rate_all > 0.5` para evitar falsos positivos en horas valle.

---

## Troubleshooting

### üî¥ PROBLEMA: WhatsApp No Recibe Mensajes

- **Diagn√≥stico:** `curl "https://.../webhooks/whatsapp?hub.mode=subscribe..."`
- **Soluci√≥n:** Verificar token, renovar SSL, revisar logs de NGINX.

### üî¥ PROBLEMA: PMS No Responde

- **Diagn√≥stico:** `docker exec agente-api ping qloapps` y `docker logs qloapps`
- **Soluci√≥n:** `docker-compose restart qloapps mysql`

---

## Runbooks

### üìò RUNBOOK: Confirmar Reserva con Se√±a

1. Verificar comprobante en dashboard.
2. Click en "Confirmar Reserva".
3. Verificar voucher enviado al cliente.
 
 ### üìò RUNBOOK: Alerta DependencyDown
 - S√≠ntoma: Alertmanager muestra "Alguna dependencia est√° ca√≠da".
 - Diagn√≥stico r√°pido:
	 1) Abrir Grafana ‚Üí Dashboard "Readiness & Dependencies".
	 2) Ver `dependency_up` para identificar cu√°l (database, redis, pms) est√° en 0.
	 3) Revisar `/health/ready` para obtener detalles.
 - Acciones sugeridas:
	 - Database: verificar contenedor `postgres`, logs y conectividad; credenciales en `.env`.
	 - Redis: verificar contenedor `redis`, salud y puertos.
	 - PMS: si `pms_type=mock`, es esperado que est√© en 1; si real, validar `PMS_BASE_URL` y disponibilidad del PMS.
 - Notas: la alerta solo dispara si hubo checks de readiness recientes (<5m) para evitar falsos positivos.

 ### üìò RUNBOOK: OrchestratorHighErrorRate
 - S√≠ntoma: Alertmanager muestra "Alta tasa de errores en Orchestrator".
 - Diagn√≥stico r√°pido:
	 1) Abrir Grafana ‚Üí Dashboard "Agente - Overview".
	 2) Revisar panel "Orchestrator error rate by intent (5m)" para identificar el intent afectado.
	 3) Ver panel "Orchestrator latency p95 by intent" y "PMS API latency p95" para correlacionar con problemas externos.
 - Acciones sugeridas:
	 - Chequear logs de la API (docker compose logs agente-api) filtrando por el intent y correlations IDs.
	 - Validar la salud del PMS (/health/ready, paneles de PMS) si el intent involucra llamadas al PMS.
	 - Si el error aparece en intents de audio, revisar el flujo de STT/TTS.

 ### üìò RUNBOOK: OrchestratorHighLatencyP95
 - S√≠ntoma: Alertmanager muestra "Latencia p95 alta en Orchestrator".
 - Diagn√≥stico r√°pido:
	 1) Grafana ‚Üí "Agente - Overview" ‚Üí panel "Orchestrator latency p95 by intent".
	 2) Correlacionar con "PMS API latency p95" y estado del Circuit Breaker.
	 3) Verificar tasa de requests (carga) y reintentos.
 - Acciones sugeridas:
	 - Escalar horizontalmente `agente-api` si la carga es sostenida.
	 - Revisar latencia y errores del PMS; aplicar circuit breaker/reintentos si no estuvieran activos.
	 - Verificar Redis y Postgres (locks/sesiones/DB) en `/health/ready`.

	 ### üìò RUNBOOK: Orchestrator SLO Degradation
	 - S√≠ntoma: Alertmanager muestra "SLO del Orchestrator en degradaci√≥n" (warning/critical).
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí "SLO Health" ‚Üí paneles "Success Rate Global" y "Top 5 Intents by Error %".
		 2) Identificar intents con peor success rate y correlacionar con paneles de error% y p95.
		 3) Revisar dependencia PMS/Redis si los intents involucrados llaman servicios externos.
	 - Acciones sugeridas:
		 - Mitigar intents problem√°ticos: degradaci√≥n controlada, respuestas de fallback.
		 - Abrir incidente si el success rate <97% por m√°s de 10m.
		 - Ajustar umbrales tras an√°lisis de tr√°fico real.
		 - Ajustar piso de tr√°fico (`orchestrator_message_rate_all`) si la alerta no dispara pese a fallos en alto volumen o dispara con volumen muy bajo.

	 ### üìò RUNBOOK: Orchestrator SLO Burn Rate
	 - S√≠ntoma: Alertmanager muestra "SLO burn rate alto/cr√≠tico".
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí "SLO Health" ‚Üí paneles de burn rate (fast/slow) y budget usado/restante.
		 2) Confirmar si el burn rate fast y slow superan umbrales (ver anotaciones de la alerta).
	 - Acciones sugeridas:
		 - Aplicar mitigaciones inmediatas en intents top-k con alto error%.
		 - Si cr√≠tico, considerar revertir despliegues recientes relacionados.
		 - Documentar impacto y consumo de error budget en el incidente.
		 - Ajustar `SLO_TARGET` (y por ende error budget) s√≥lo tras an√°lisis post-mortem, nunca durante un incidente activo.

	 ### üìò RUNBOOK: HighHttp5xxRate

	 ### üìò RUNBOOK: SLO Budget Exhaust Forecast
	 - S√≠ntoma: Alertmanager muestra proyecci√≥n de agotamiento (<12h o <6h).
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí "SLO Health" ‚Üí panel "Hours to Exhaust Budget".
		 2) Verificar burn rates y budget used.
		 3) Identificar intents top error% / p95.
	 - Acciones sugeridas:
		 - Iniciar acciones de mitigaci√≥n (reducci√≥n de features, fallback responses).
		 - Si cr√≠tico (<6h) escalar a guardia e iniciar plan de reducci√≥n de errores priorizando intents top.
		 - Evaluar si hay despliegue reciente correlacionado.
	 - S√≠ntoma: Alertmanager muestra "Alta tasa de 5xx" en un endpoint.
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí "Agente - Overview" ‚Üí panel "HTTP 5xx rate (5m)".
		 2) Revisar logs de `agente-api` y NGINX para ese endpoint.
		 3) Correlacionar con "Orchestrator error percentage" si aplica.
	 - Acciones sugeridas:
		 - Identificar excepciones frecuentes en logs y abrir issue.
		 - Validar payloads de entrada (sanitizaci√≥n/validaci√≥n) y dependencias externas.
		 - Implementar manejo de errores y tests si faltan.

	 ### üìò RUNBOOK: HighPmsLatencyP95
	 - S√≠ntoma: p95 de PMS > umbral sostenido.
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí panel "PMS API latency p95".
		 2) Verificar estado del PMS (servicios `qloapps`/`mysql`).
		 3) Revisar circuit breaker y reintentos.
	 - Acciones sugeridas:
		 - Aumentar caching en el adapter; validar Redis.
		 - Ajustar timeouts/backoff del adapter.
		 - Coordinar con el equipo del PMS.

	 ### üìò RUNBOOK: PmsCircuitBreakerImminentOpen
	 - S√≠ntoma: Alertas `PmsCircuitBreakerImminentOpenWarning` o `PmsCircuitBreakerImminentOpenCritical` indicando riesgo de apertura.
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí panel "Circuit Breaker state" y a√±adir paneles derivados (failure ratios, streak) si no existen.
		 2) Ver panel "PMS API latency p95" y "PMS Errors by type" para correlacionar naturaleza (timeouts vs 5xx).
		 3) Consultar m√©tricas: `pms_cb_failure_streak_fraction`, `pms_cb_failure_ratio_1m`, `pms_cb_minutes_to_open_estimate`.
	 - Posibles causas:
		 - Degradaci√≥n real del PMS (timeouts incrementales).
		 - Cambios recientes en timeouts o thresholds del adapter.
		 - Pico de tr√°fico con patrones no cacheados (incrementa presi√≥n y fallos).
	 - Acciones sugeridas (orden):
		 1) Confirmar que los fallos son leg√≠timos revisando logs (buscar patrones repetidos).
		 2) Si los fallos son timeouts ‚Üí aumentar temporalmente `read` timeout (ej. +50%) y monitorear efecto.
		 3) Activar rutas de degradaci√≥n: limitar intents que disparan llamadas PMS no cr√≠ticas.
		 4) Incrementar TTL de cache de disponibilidad para reducir presi√≥n sobre PMS.
		 5) Coordinar con equipo PMS si latencia/errores se originan upstream.
	 - Mitigaci√≥n preventiva:
		 - Si `pms_cb_minutes_to_open_estimate < 2` y la racha sigue subiendo, aplicar pasos 2‚Äì4 antes de que el breaker abra.
	 - Post-mortem:
		 - Evaluar si `failure_threshold=5` es demasiado bajo para el perfil de error transitorio.
		 - Considerar backoff m√°s agresivo para retries o circuit half-open probabil√≠stico.

	 ### üìò RUNBOOK: CircuitBreakerOpen
	 - S√≠ntoma: Circuit Breaker abierto por m√°s de 2m.
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí panel "Circuit Breaker state" (rojo cuando >0).
		 2) Correlacionar con latencia/errores del PMS.
		 3) Logs del adapter para ver causas (rate limit, timeouts, 5xx).
	 - Acciones sugeridas:
		 - Verificar credenciales/endpoints del PMS.
		 - Incrementar l√≠mites/retrys temporalmente si corresponde.
		 - Desplegar mitigaciones (cache warmup, degradaci√≥n controlada).

	 ### üìò RUNBOOK: PmsCacheHitRatio
	 - S√≠ntoma: Alertas `PmsCacheHitRatioLowWarning` o `PmsCacheHitRatioLowCritical`.
	 - Diagn√≥stico r√°pido:
		 1) Grafana ‚Üí Dashboard "Agente - Overview" ‚Üí panel 21 (hit ratio) y panel 22 (hits vs misses).
		 2) Correlacionar con panel de latencia PMS p95 y estado del Circuit Breaker.
		 3) Verificar en logs si hay patr√≥n de invalidaciones frecuentes (`Invalidated ... cache keys`).
	 - Posibles causas:
		 - TTL demasiado corto (expiraciones antes de reutilizaci√≥n real).
		 - Clave de cache con demasiados par√°metros (alta cardinalidad) ‚Üí baja reutilizaci√≥n.
		 - Invalidation agresiva tras `create_reservation` u otras operaciones de escritura.
		 - Pico de nuevos tipos de consultas (cambio de tr√°fico estacional) a√∫n no calientes.
	 - Acciones sugeridas (orden):
		 1) Confirmar volumen: asegurar actividad >0.2 ops/s (condici√≥n de la alerta).
		 2) Inspeccionar keys representativas en Redis (opcional) para patrones de cardinalidad.
		 3) Ajustar TTL (aumentar) temporalmente si expiraci√≥n temprana es evidente.
		 4) Implementar cache pre-warm (script) para queries populares de disponibilidad (fechas cercanas, room types top).
		 5) Revisar l√≥gica de invalidaci√≥n: evaluar un patr√≥n m√°s espec√≠fico en lugar de `availability:*` completo.
		 6) Si latencia PMS tambi√©n aumenta ‚Üí priorizar mitigaci√≥n y escalar a equipo PMS.
	 - M√©trica clave:
		 `pms_cache_hit_ratio` (recording rule 5m). Objetivo recomendado inicial: >0.8.
	 - Post-mortem:
		 Documentar ajustes (TTL, patrones clave, warm-up) y validar mejora sostenida >24h.

---

## Mantenimiento

### Semanal (Domingos 04:00)

- Limpiar logs antiguos.
- Optimizar base de datos: `docker exec postgres vacuumdb ...`

---

## Recuperaci√≥n de Desastres

### Falla Total del Servidor

1. Provisionar nuevo servidor.
2. Restaurar √∫ltimo backup desde S3.
3. Ejecutar `./scripts/restore_backup.sh`.
4. Actualizar DNS.
